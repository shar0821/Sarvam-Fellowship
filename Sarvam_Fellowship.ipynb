{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Install Libraries"
      ],
      "metadata": {
        "id": "AK6TPehN5bpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRV0Y2MoxhsC",
        "outputId": "adcf30a7-c85d-4687-954d-dd960afaae58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m649.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Process Data"
      ],
      "metadata": {
        "id": "uk8iY4BX5fx9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8DRDtxvwM2S"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "def load_top_words(path, top_k=100000):\n",
        "    model = KeyedVectors.load_word2vec_format(path, limit=top_k)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_lexicon(path, src_model, tgt_model):\n",
        "    pairs = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            src, tgt = line.strip().split()\n",
        "            if src in src_model and tgt in tgt_model:\n",
        "                pairs.append((src, tgt))\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "rirFIx9FwO_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_embedding_matrices(pairs, src_model, tgt_model):\n",
        "    src_matrix = []\n",
        "    tgt_matrix = []\n",
        "    for src, tgt in pairs:\n",
        "        src_matrix.append(src_model[src])\n",
        "        tgt_matrix.append(tgt_model[tgt])\n",
        "    return np.array(src_matrix).T, np.array(tgt_matrix).T"
      ],
      "metadata": {
        "id": "eeoUkAYxwTO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def procrustes(X, Y):\n",
        "    # Compute optimal orthogonal mapping W\n",
        "    U, _, Vt = np.linalg.svd(Y @ X.T)\n",
        "    W = U @ Vt\n",
        "    return W"
      ],
      "metadata": {
        "id": "0BPz-pVYwVzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_mapping(src_model, W):\n",
        "    mapped_vectors = {word: W @ src_model[word] for word in src_model.key_to_index}\n",
        "    return mapped_vectors"
      ],
      "metadata": {
        "id": "P0wq-jKtwdsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def translate_words(src_words, mapped_src_vectors, tgt_model, top_k=5):\n",
        "    results = {}\n",
        "    tgt_matrix = np.array([tgt_model[word] for word in tgt_model.key_to_index])\n",
        "    tgt_words = list(tgt_model.key_to_index.keys())\n",
        "\n",
        "    for word in src_words:\n",
        "        if word in mapped_src_vectors:\n",
        "            vec = mapped_src_vectors[word].reshape(1, -1)\n",
        "            sims = cosine_similarity(vec, tgt_matrix).flatten()\n",
        "            top_k_indices = np.argsort(-sims)[:top_k]\n",
        "            results[word] = [tgt_words[i] for i in top_k_indices]\n",
        "    return results"
      ],
      "metadata": {
        "id": "NCzhFB-Lwi--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(results, gold_dict, k=1):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for word in results:\n",
        "        if word in gold_dict:\n",
        "            total += 1\n",
        "            correct += int(gold_dict[word] in results[word][:k])\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "IHEobAsPwluj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Text Embeddings"
      ],
      "metadata": {
        "id": "vErsSgPb5TDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_model = load_top_words('/content/drive/MyDrive/cc.en.300.vec')\n",
        "hi_model = load_top_words('/content/drive/MyDrive/cc.hi.300.vec')"
      ],
      "metadata": {
        "id": "OmJEE-0Ty2Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bilingual_lexicon = load_lexicon('en-hi.txt', en_model, hi_model)"
      ],
      "metadata": {
        "id": "AQmNF_9lzhKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_embedding_matrices(bilingual_lexicon, en_model, hi_model)"
      ],
      "metadata": {
        "id": "BwVWnuDrzjfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perform Procrustes Method to find similarity"
      ],
      "metadata": {
        "id": "axhakw1x5kYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import procrustes\n",
        "\n",
        "def scipy_procrustes(X, Y):\n",
        "    \"\"\"Aligns X to Y using Scipy's built-in Procrustes method\"\"\"\n",
        "    X_aligned, Y_aligned, disparity = procrustes(X.T, Y.T)  # Transpose back to original format\n",
        "    return X_aligned.T, disparity  # Return transposed back\n",
        "X_procrustes, disparity = scipy_procrustes(X, Y)"
      ],
      "metadata": {
        "id": "j4TMX2F41FPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = procrustes(X, Y)"
      ],
      "metadata": {
        "id": "SotVR8ADznW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapped_en_vectors = apply_mapping(en_model, W)"
      ],
      "metadata": {
        "id": "yMtBug9Zzq92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_words = [\"cat\", \"house\", \"river\"]\n",
        "translations = translate_words(test_words, mapped_en_vectors, hi_model, top_k=5)\n",
        "\n",
        "for word, trans in translations.items():\n",
        "    print(f\"{word} -> {trans}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrEb2bndzx8_",
        "outputId": "c6a3a51b-30c3-44fd-9c46-347bc12b4525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat -> ['बिल्ली', 'कुत्ता', 'कुत्ते', 'पालतू', 'कुत्तों']\n",
            "house -> ['मकान', 'फ़्लैट', 'घर', 'कमरे', 'हाउस']\n",
            "river -> ['नदी', 'किनारे', 'गंगा', 'चनाब', 'नहर']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results"
      ],
      "metadata": {
        "id": "AclRwuOy5th3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gold_dict = {src: tgt for src, tgt in bilingual_lexicon}\n",
        "\n",
        "# Compute Precision@1 and Precision@5\n",
        "p1 = precision_at_k(translations, gold_dict, k=1)\n",
        "p5 = precision_at_k(translations, gold_dict, k=5)\n",
        "\n",
        "print(f\"Precision@1: {p1:.4f}\")\n",
        "print(f\"Precision@5: {p5:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRg4bG9uzy_p",
        "outputId": "80b13b18-e444-4a23-f728-146c7841046e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@1: 0.6667\n",
            "Precision@5: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [5000, 10000, 20000]\n",
        "for size in sizes:\n",
        "    small_lexicon = bilingual_lexicon[:size]\n",
        "    X_small, Y_small = get_embedding_matrices(small_lexicon, en_model, hi_model)\n",
        "    W_small = procrustes(X_small, Y_small)\n",
        "    mapped_small = apply_mapping(en_model, W_small)\n",
        "    translations_small = translate_words(test_words, mapped_small, hi_model, top_k=5)\n",
        "    p1_small = precision_at_k(translations_small, gold_dict, k=1)\n",
        "    p5_small = precision_at_k(translations_small, gold_dict, k=5)\n",
        "    print(f\"Dictionary Size: {size}, Precision@1: {p1_small:.4f}, Precision@5: {p5_small:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bN4ub0Wz9J9",
        "outputId": "0fd494cb-3e9a-47ee-d83e-d67e5de98ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary Size: 5000, Precision@1: 0.6667, Precision@5: 1.0000\n",
            "Dictionary Size: 10000, Precision@1: 0.6667, Precision@5: 0.6667\n",
            "Dictionary Size: 20000, Precision@1: 0.6667, Precision@5: 1.0000\n"
          ]
        }
      ]
    }
  ]
}